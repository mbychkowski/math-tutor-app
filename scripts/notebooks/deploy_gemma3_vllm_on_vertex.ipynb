{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Deploy Gemma 3 to Vertex AI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates deploying Gemma 3 models on GPU using [vLLM](https://github.com/vllm-project/vllm).\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Deploy Gemma 3 with vLLM on GPU\n",
    "\n",
    "### File a bug\n",
    "\n",
    "File a bug on [GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/new) if you encounter any issue with the notebook.\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "- Install Google Cloud CLI\n",
    "- Create a .evn file with following eavlues\n",
    "\n",
    "```\n",
    "PROJECT_ID = \"\"\n",
    "REGION = \"\"\n",
    "MODEL_BUCKET = \" \"\n",
    "MODEL_NAME =\" \"\n",
    "MODEL_VERSION=\" \"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Warning:** Please make sure python version is 3.12 or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (3.10.5)\n",
      "Requirement already satisfied: python-dotenv in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.103.0'\n",
    "! pip3 install matplotlib python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # This loads the variables from .env into the environment\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "MODEL_BUCKET = os.getenv(\"MODEL_BUCKET\")\n",
    "MODEL_NAME =os.getenv(\"MODEL_NAME\")\n",
    "MODEL_VERSION=os.getenv(\"MODEL_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_MODEL_PATH = f\"gs://{MODEL_BUCKET}/{MODEL_NAME}/{MODEL_VERSION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "executionInfo": {
     "elapsed": 59079,
     "status": "ok",
     "timestamp": 1754233459931,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "2707b02ef5df",
    "outputId": "c65ca5e6-6155-4af4-f14c-8c25574ef3e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from tensorflow) (6.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from tensorflow) (2.32.5)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from tensorflow) (4.15.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.3-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from tensorflow) (1.74.0)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from tensorflow) (2.3.2)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: rich in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/mbychkowski/code/math-tutor-app/.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.8/620.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0mm\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.14.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wrapt-1.17.3-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (414 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorboard-data-server, setuptools, optree, opt_einsum, ml_dtypes, markdown, h5py, google_pasta, gast, absl-py, tensorboard, astunparse, keras, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/21\u001b[0m [tensorflow]1\u001b[0m [tensorflow]]data-server]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google_pasta-0.2.0 h5py-3.14.0 keras-3.11.3 libclang-18.1.1 markdown-3.8.2 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 setuptools-80.9.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 19:46:20.817571: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-28 19:46:20.817983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-28 19:46:20.881402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-28 19:46:22.517828: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-28 19:46:22.518691: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-933718959305-fdcb0bf5-4671-4bf0-a34b-836e0aedbeb1\" finished successfully.\n",
      "Initializing Vertex AI API.\n",
      "Using this default Service Account: 933718959305-compute@developer.gserviceaccount.com\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Google Cloud project\n",
    "\n",
    "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "# @markdown 2. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
    "\n",
    "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
    "# @markdown | ----------- | ----------- | ----------- |\n",
    "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
    "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
    "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
    "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
    "\n",
    "# Upgrade Vertex AI SDK.\n",
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.103.0'\n",
    "\n",
    "# Import the necessary packages\n",
    "import importlib\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Upgrade Vertex AI SDK.\n",
    "if os.environ.get(\"VERTEX_PRODUCT\") != \"COLAB_ENTERPRISE\":\n",
    "    ! pip install --upgrade tensorflow\n",
    "#! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"common_util\"\n",
    ")\n",
    "\n",
    "# Initialize models and endpoints as a dict\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "if not REGION:\n",
    "    REGION = os.environ[\"REGION\"]\n",
    "\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFfqpQm8BNwZ"
   },
   "source": [
    "## Deploy Gemma 3 1B models with vLLM on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 127019,
     "status": "ok",
     "timestamp": 1754233586947,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "W1PYsnz2JXHm"
   },
   "outputs": [],
   "source": [
    "# @markdown Set the model to deploy.\n",
    "import importlib\n",
    "common_util = importlib.import_module(\n",
    "    \"common_util\"\n",
    ")\n",
    "\n",
    "base_model_name = \"gemma-3-1b-it\"  # @param [\"gemma-3-1b-pt\", \"gemma-3-1b-it\"] {isTemplate:true}\n",
    "# hf_model_id = \"google/\" + base_model_name  # uncomment if deploying from Hugging Face\n",
    "PUBLISHER_MODEL_NAME = f\"publishers/google/models/gemma3@{base_model_name}\"\n",
    "#model_id = f\"gs://vertex-model-garden-restricted-us/gemma3/{base_model_name}\"\n",
    "model_id =GCS_MODEL_PATH # If deploy from HF, model_id should be hf_model_id.\n",
    "\n",
    "# The pre-built serving docker image.\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:latest\"\n",
    "\n",
    "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
    "use_dedicated_endpoint = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "machine_type = \"g2-standard-48\"\n",
    "accelerator_count = 4\n",
    "\n",
    "# common_util.check_quota(\n",
    "#     project_id=PROJECT_ID,\n",
    "#     region=REGION,\n",
    "#     accelerator_type=accelerator_type,\n",
    "#     accelerator_count=accelerator_count,\n",
    "#     is_for_training=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"creationTimestamp\": \"1969-12-31T16:00:00.000-08:00\",\n",
      "  \"description\": \"us-central1\",\n",
      "  \"id\": \"1000\",\n",
      "  \"kind\": \"compute#region\",\n",
      "  \"name\": \"us-central1\",\n",
      "  \"quotas\": [\n",
      "    {\n",
      "      \"limit\": 200.0,\n",
      "      \"metric\": \"CPUS\",\n",
      "      \"usage\": 18.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 40960.0,\n",
      "      \"metric\": \"DISKS_TOTAL_GB\",\n",
      "      \"usage\": 170.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 5000.0,\n",
      "      \"metric\": \"SNAPSHOTS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 21.0,\n",
      "      \"metric\": \"STATIC_ADDRESSES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 69.0,\n",
      "      \"metric\": \"IN_USE_ADDRESSES\",\n",
      "      \"usage\": 13.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 20480.0,\n",
      "      \"metric\": \"SSD_TOTAL_GB\",\n",
      "      \"usage\": 1300.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 300.0,\n",
      "      \"metric\": \"INSTANCE_TEMPLATES\",\n",
      "      \"usage\": 4.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 9.223372036854776e+18,\n",
      "      \"metric\": \"LOCAL_SSD_TOTAL_GB\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 300.0,\n",
      "      \"metric\": \"INSTANCE_GROUPS\",\n",
      "      \"usage\": 4.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 300.0,\n",
      "      \"metric\": \"INSTANCE_GROUP_MANAGERS\",\n",
      "      \"usage\": 4.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 720.0,\n",
      "      \"metric\": \"INSTANCES\",\n",
      "      \"usage\": 20.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 150.0,\n",
      "      \"metric\": \"AUTOSCALERS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"REGIONAL_AUTOSCALERS\",\n",
      "      \"usage\": 1.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 300.0,\n",
      "      \"metric\": \"REGIONAL_INSTANCE_GROUP_MANAGERS\",\n",
      "      \"usage\": 1.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 30.0,\n",
      "      \"metric\": \"TARGET_TCP_PROXIES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"PREEMPTIBLE_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 8.0,\n",
      "      \"metric\": \"NVIDIA_K80_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_LOCAL_SSD_TOTAL_GB\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITMENTS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1000.0,\n",
      "      \"metric\": \"NETWORK_ENDPOINT_GROUPS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 5000.0,\n",
      "      \"metric\": \"INTERNAL_ADDRESSES\",\n",
      "      \"usage\": 2.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"NVIDIA_P100_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"PREEMPTIBLE_LOCAL_SSD_GB\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 30.0,\n",
      "      \"metric\": \"SSL_POLICIES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_K80_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_P100_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"NVIDIA_P100_VWS_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"NVIDIA_V100_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"NVIDIA_P4_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"NVIDIA_P4_VWS_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"NODE_GROUPS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"NODE_TEMPLATES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_V100_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_P4_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_P100_VWS_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_P4_VWS_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 16.0,\n",
      "      \"metric\": \"INTERCONNECT_ATTACHMENTS_PER_REGION\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 80000.0,\n",
      "      \"metric\": \"INTERCONNECT_ATTACHMENTS_TOTAL_MBPS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 250.0,\n",
      "      \"metric\": \"RESOURCE_POLICIES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 50.0,\n",
      "      \"metric\": \"IN_USE_SNAPSHOT_SCHEDULES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 4.0,\n",
      "      \"metric\": \"NVIDIA_T4_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 2.0,\n",
      "      \"metric\": \"NVIDIA_T4_VWS_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 4.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_T4_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 2.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_T4_VWS_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 50.0,\n",
      "      \"metric\": \"IN_USE_BACKUP_SCHEDULES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 10.0,\n",
      "      \"metric\": \"PUBLIC_DELEGATED_PREFIXES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_NVIDIA_K80_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_NVIDIA_P100_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_NVIDIA_P4_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_NVIDIA_V100_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_NVIDIA_T4_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"C2_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 200.0,\n",
      "      \"metric\": \"N2_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_N2_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_C2_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"RESERVATIONS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_LICENSES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 200.0,\n",
      "      \"metric\": \"N2D_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_N2D_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 600.0,\n",
      "      \"metric\": \"SERVICE_ATTACHMENTS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1024.0,\n",
      "      \"metric\": \"STATIC_BYOIP_ADDRESSES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 9.223372036854776e+18,\n",
      "      \"metric\": \"AFFINITY_GROUPS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1.0,\n",
      "      \"metric\": \"NVIDIA_A100_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 16.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_A100_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_NVIDIA_A100_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 640.0,\n",
      "      \"metric\": \"M1_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"M2_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 12.0,\n",
      "      \"metric\": \"A2_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_A2_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_MEMORY_OPTIMIZED_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 30.0,\n",
      "      \"metric\": \"NETWORK_FIREWALL_POLICIES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 600.0,\n",
      "      \"metric\": \"PSC_INTERNAL_LB_FORWARDING_RULES\",\n",
      "      \"usage\": 2.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"EXTERNAL_NETWORK_LB_FORWARDING_RULES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"EXTERNAL_PROTOCOL_FORWARDING_RULES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 540000.0,\n",
      "      \"metric\": \"PD_EXTREME_TOTAL_PROVISIONED_IOPS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 72.0,\n",
      "      \"metric\": \"E2_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_E2_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 25.0,\n",
      "      \"metric\": \"EXTERNAL_MANAGED_FORWARDING_RULES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"C2D_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_C2D_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 24.0,\n",
      "      \"metric\": \"N2A_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 10.0,\n",
      "      \"metric\": \"SECURITY_POLICIES_PER_REGION\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"SECURITY_POLICY_RULES_PER_REGION\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 24.0,\n",
      "      \"metric\": \"T2D_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 24.0,\n",
      "      \"metric\": \"COMMITTED_T2D_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 24.0,\n",
      "      \"metric\": \"C3_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_C3_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 96.0,\n",
      "      \"metric\": \"T2A_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 248.0,\n",
      "      \"metric\": \"M3_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_M3_CPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"NVIDIA_A100_80GB_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_A100_80GB_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"COMMITTED_NVIDIA_A100_80GB_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 600.0,\n",
      "      \"metric\": \"NETWORK_ATTACHMENTS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 75.0,\n",
      "      \"metric\": \"REGIONAL_INTERNAL_MANAGED_BACKEND_SERVICES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 75.0,\n",
      "      \"metric\": \"REGIONAL_EXTERNAL_MANAGED_BACKEND_SERVICES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"REGIONAL_EXTERNAL_NETWORK_LB_BACKEND_SERVICES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 75.0,\n",
      "      \"metric\": \"REGIONAL_INTERNAL_LB_BACKEND_SERVICES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"REGIONAL_INTERNAL_TRAFFIC_DIRECTOR_BACKEND_SERVICES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 10.0,\n",
      "      \"metric\": \"NET_LB_SECURITY_POLICIES_PER_REGION\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 100.0,\n",
      "      \"metric\": \"NET_LB_SECURITY_POLICY_RULES_PER_REGION\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1000.0,\n",
      "      \"metric\": \"NET_LB_SECURITY_POLICY_RULE_ATTRIBUTES_PER_REGION\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"TPU_LITE_DEVICE_V5\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"PREEMPTIBLE_TPU_LITE_DEVICE_V5\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 32.0,\n",
      "      \"metric\": \"TPU_LITE_PODSLICE_V5\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 8.0,\n",
      "      \"metric\": \"NVIDIA_L4_GPUS\",\n",
      "      \"usage\": 2.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 8.0,\n",
      "      \"metric\": \"PREEMPTIBLE_NVIDIA_L4_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 8.0,\n",
      "      \"metric\": \"COMMITTED_NVIDIA_L4_GPUS\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 5000.0,\n",
      "      \"metric\": \"STATIC_EXTERNAL_IPV6_ADDRESS_RANGES\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 20.0,\n",
      "      \"metric\": \"SECURITY_POLICY_ADVANCED_RULES_PER_REGION\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 1536.0,\n",
      "      \"metric\": \"PREEMPTIBLE_TPU_LITE_PODSLICE_V5\",\n",
      "      \"usage\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"limit\": 0.0,\n",
      "      \"metric\": \"VARIABLE_IPV6_PUBLIC_DELEGATED_PREFIXES\",\n",
      "      \"usage\": 0.0\n",
      "    }\n",
      "  ],\n",
      "  \"selfLink\": \"https://www.googleapis.com/compute/v1/projects/prj-kokiri-dev/regions/us-central1\",\n",
      "  \"status\": \"UP\",\n",
      "  \"supportsPzs\": false,\n",
      "  \"zones\": [\n",
      "    \"https://www.googleapis.com/compute/v1/projects/prj-kokiri-dev/zones/us-central1-a\",\n",
      "    \"https://www.googleapis.com/compute/v1/projects/prj-kokiri-dev/zones/us-central1-b\",\n",
      "    \"https://www.googleapis.com/compute/v1/projects/prj-kokiri-dev/zones/us-central1-c\",\n",
      "    \"https://www.googleapis.com/compute/v1/projects/prj-kokiri-dev/zones/us-central1-f\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! gcloud compute regions describe us-central1   --project={PROJECT_ID}   --format=\"get(quotas[?metric=='{accelerator_type}_GPUS'])\" --format=json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260243,
     "status": "ok",
     "timestamp": 1754233847187,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "TBNJYZMlBNwZ",
    "outputId": "ecc032c7-42e8-4c9e-ff47-b81a9d97dc60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/933718959305/locations/us-central1/endpoints/5651563434047700992/operations/3255510185879273472\n",
      "Endpoint created. Resource name: projects/933718959305/locations/us-central1/endpoints/5651563434047700992\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/933718959305/locations/us-central1/endpoints/5651563434047700992')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/933718959305/locations/us-central1/models/2388116165785288704/operations/8442531056703242240\n",
      "Model created. Resource name: projects/933718959305/locations/us-central1/models/2388116165785288704@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/933718959305/locations/us-central1/models/2388116165785288704@1')\n",
      "Deploying gemma3-serve-20250828-194633 on g2-standard-48 with 4 NVIDIA_L4 GPU(s).\n",
      "Deploying model to Endpoint : projects/933718959305/locations/us-central1/endpoints/5651563434047700992\n",
      "Deploy Endpoint model backing LRO: projects/933718959305/locations/us-central1/endpoints/5651563434047700992/operations/4408431690486120448\n"
     ]
    }
   ],
   "source": [
    "# @title Deploy with customized configs\n",
    "\n",
    "# @markdown This section uploads Gemma 3 1B models to Model Registry and deploys them to a Vertex Prediction Endpoint. It takes 15 minutes to 30 minutes to finish.\n",
    "\n",
    "gpu_memory_utilization = 0.95\n",
    "max_model_len = 32768\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    publisher: str,\n",
    "    publisher_model_id: str,\n",
    "    base_model_id: str = None,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    gpu_memory_utilization: float = 0.9,\n",
    "    max_model_len: int = 4096,\n",
    "    dtype: str = \"auto\",\n",
    "    enable_trust_remote_code: bool = False,\n",
    "    enforce_eager: bool = False,\n",
    "    enable_lora: bool = False,\n",
    "    enable_chunked_prefill: bool = False,\n",
    "    enable_prefix_cache: bool = False,\n",
    "    host_prefix_kv_cache_utilization_target: float = 0.0,\n",
    "    max_loras: int = 1,\n",
    "    max_cpu_loras: int = 8,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    "    max_num_seqs: int = 256,\n",
    "    model_type: str = None,\n",
    "    enable_llama_tool_parser: bool = False,\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=f\"{model_name}-endpoint\",\n",
    "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "    )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
    "    vllm_args = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.api_server\",\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=8080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        f\"--max-loras={max_loras}\",\n",
    "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
    "        f\"--max-num-seqs={max_num_seqs}\",\n",
    "        \"--disable-log-stats\",\n",
    "        \"--enable-auto-tool-choice\",\n",
    "        \"--tool-call-parser=pythonic\"\n",
    "    ]\n",
    "\n",
    "    if enable_trust_remote_code:\n",
    "        vllm_args.append(\"--trust-remote-code\")\n",
    "\n",
    "    if enforce_eager:\n",
    "        vllm_args.append(\"--enforce-eager\")\n",
    "\n",
    "    if enable_lora:\n",
    "        vllm_args.append(\"--enable-lora\")\n",
    "\n",
    "    if enable_chunked_prefill:\n",
    "        vllm_args.append(\"--enable-chunked-prefill\")\n",
    "\n",
    "    if enable_prefix_cache:\n",
    "        vllm_args.append(\"--enable-prefix-caching\")\n",
    "\n",
    "    if 0 < host_prefix_kv_cache_utilization_target < 1:\n",
    "        vllm_args.append(\n",
    "            f\"--host-prefix-kv-cache-utilization-target={host_prefix_kv_cache_utilization_target}\"\n",
    "        )\n",
    "\n",
    "    if model_type:\n",
    "        vllm_args.append(f\"--model-type={model_type}\")\n",
    "\n",
    "    if enable_llama_tool_parser:\n",
    "        vllm_args.append(\"--enable-auto-tool-choice\")\n",
    "        vllm_args.append(\"--tool-call-parser=vertex-llama-3\")\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[8080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        model_garden_source_model_name=(\n",
    "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
    "    )\n",
    "\n",
    "    model.predict_schemata.prediction_schema_uri = f\"gs://{MODEL_BUCKET}/{MODEL_NAME}/{MODEL_VERSION}/output.yaml\"\n",
    "    model.update()\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=3600,\n",
    "\n",
    "        spot=False,\n",
    "\n",
    "        system_labels={\n",
    "            \"NOTEBOOK_NAME\": \"model_garden_gemma3_deployment_on_vertex.ipynb\",\n",
    "            \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
    "        },\n",
    "    )\n",
    "    print(\"endpoint_name:\", endpoint.name)\n",
    "\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "LABEL = \"custom-deploy-1b\"\n",
    "\n",
    "models[LABEL], endpoints[LABEL] = deploy_model_vllm(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=\"gemma3-serve\"),\n",
    "    model_id=model_id,\n",
    "    publisher=\"google\",\n",
    "    publisher_model_id=\"gemma3\",\n",
    "    # base_model_id=hf_model_id,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    max_model_len=max_model_len,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")\n",
    "\n",
    "model = models[LABEL]\n",
    "endpoint = endpoints[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1753825040558,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "rDHsCOqvFYBi",
    "outputId": "0fae35e6-a7c0-4f5a-b01b-d8dbbb2d8e9e"
   },
   "outputs": [],
   "source": [
    "# @title Raw predict\n",
    "\n",
    "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
    "\n",
    "# @markdown Example:\n",
    "\n",
    "# @markdown ```\n",
    "# @markdown Human: What is a car?\n",
    "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "# @markdown ```\n",
    "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
    "\n",
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
    "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
    "max_tokens = 50  # @param {type:\"integer\"}\n",
    "temperature = 1.0  # @param {type:\"number\"}\n",
    "top_p = 1.0  # @param {type:\"number\"}\n",
    "top_k = 1  # @param {type:\"integer\"}\n",
    "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
    "raw_response = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "response = endpoint.predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n",
    "\n",
    "# @markdown Click \"Show Code\" to see more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10478,
     "status": "ok",
     "timestamp": 1753825593471,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "LSG9ITWTbTb7",
    "outputId": "5e9f2ae1-f1ff-4a64-8472-1558807accaa"
   },
   "outputs": [],
   "source": [
    "# @title Chat completion\n",
    "\n",
    "if use_dedicated_endpoint:\n",
    "    DEDICATED_ENDPOINT_DNS = endpoint.gca_resource.dedicated_endpoint_dns\n",
    "ENDPOINT_RESOURCE_NAME = endpoint.resource_name\n",
    "\n",
    "# @title Chat Completions Inference\n",
    "\n",
    "# @markdown Once deployment succeeds, you can send requests to the endpoint using the OpenAI SDK.\n",
    "\n",
    "# @markdown First you will need to install the SDK and some auth-related dependencies.\n",
    "\n",
    "! pip install -qU openai google-auth requests\n",
    "\n",
    "# @markdown Next fill out some request parameters:\n",
    "\n",
    "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
    "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
    "max_tokens = 50  # @param {type: \"integer\"}\n",
    "temperature = 1.0  # @param {type: \"number\"}\n",
    "stream = False  # @param {type: \"boolean\"}\n",
    "\n",
    "# @markdown Now we can send a request.\n",
    "\n",
    "import google.auth\n",
    "import openai\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "\n",
    "BASE_URL = (\n",
    "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    ")\n",
    "try:\n",
    "    if use_dedicated_endpoint:\n",
    "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
    "\n",
    "model_response = client.chat.completions.create(\n",
    "    model=\"\",\n",
    "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    stream=stream,\n",
    ")\n",
    "\n",
    "if stream:\n",
    "    usage = None\n",
    "    contents = []\n",
    "    for chunk in model_response:\n",
    "        if chunk.usage is not None:\n",
    "            usage = chunk.usage\n",
    "            continue\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n",
    "        contents.append(chunk.choices[0].delta.content)\n",
    "    print(f\"\\n\\n{usage}\")\n",
    "else:\n",
    "    print(model_response)\n",
    "\n",
    "# @markdown Click \"Show Code\" to see more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# @title Delete the models and endpoints\n",
    "\n",
    "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
    "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "for endpoint in endpoints.values():\n",
    "    endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "for model in models.values():\n",
    "    model.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_gemma3_deployment_on_vertex_dw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
